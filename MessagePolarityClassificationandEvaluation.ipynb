{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import collections\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from  nltk.stem import SnowballStemmer\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "def processTweet(tweet):\n",
    "    # Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Remove url\n",
    "    tweet = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
    "    # Remove hashtags\n",
    "    # only removing the hash # sign from the word, we believe hashtags contains useful information\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Remove HTML special entities (e.g. amp;)\n",
    "    tweet = re.sub(r'\\\\w*;', '', tweet)\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+', '', tweet)\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    # replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "# Check if a string has a number\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def pre_process_for_model(file,output):\n",
    "    # load input data line by line with utf8\n",
    "    \n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        # If there is no data in the file, don't go any further.\n",
    "        if not len(lines):\n",
    "            exit\n",
    "      \n",
    "  \n",
    "\n",
    "    # get a dictionary of each tweet by regex\n",
    "    tweets_dict = {\"tweet_id\": [], \"tweet_sentiment\": [], \"tweet_content\": []}\n",
    "    for line in lines:\n",
    "        matches = re.compile(r'(^\\d*)(\\s)([^\\s]+)(\\s)(.*)')\n",
    "        tweets_dict[\"tweet_id\"].append(matches.match(line).group(1))\n",
    "        tweets_dict[\"tweet_sentiment\"].append(matches.match(line).group(3))\n",
    "        tweets_dict[\"tweet_content\"].append(matches.match(line).group(5))\n",
    "\n",
    "    # create a data frame of tweet\n",
    "    tweets_df = pd.DataFrame(tweets_dict)\n",
    "\n",
    "    # report the number of tweet by checking the unique tweet id (no duplicated id found)\n",
    "   \n",
    "\n",
    "    # write all tweets into one text\n",
    "    tweets_list = tweets_df['tweet_content'].tolist()\n",
    "\n",
    "    # sum up the length of each line\n",
    "    num_of_character = sum(len(list) for list in tweets_list)\n",
    "    \n",
    "    \n",
    "    tolist=[]\n",
    "\n",
    "        \n",
    "    tweets_df['clean_text'] = tweets_df['tweet_content'].apply(processTweet)\n",
    "    tweets_df['token'] = tweets_df['clean_text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    \n",
    "    tweets_df.to_csv(output, index=None, header=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "##normalize text data\n",
    "def stem_stop(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text =  str(text).lower()\n",
    "    #text=nltk.word_tokenize(text)\n",
    "    text=re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = []\n",
    "     \n",
    "    for token in text.split():\n",
    "        \n",
    "        \n",
    "        if wordnet.synsets(token) and hasNumbers(token)==False:\n",
    "            if token not in stop_words:\n",
    "                if stem:\n",
    "                 \n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "##navie bayes\n",
    "    \n",
    "def NB(df_train,df_dev):\n",
    "    \n",
    "    # Feature extraction\n",
    "    #n=1200\n",
    "    df_train['clean_text']=df_train['clean_text'].apply(lambda x: stem_stop(x))\n",
    "    df_dev['clean_text']=df_dev['clean_text'].apply(lambda x: stem_stop(x))\n",
    "    \n",
    "    df_pos_train = df_train[df_train['tweet_sentiment'] == 'positive']\n",
    "    #df_pos_train= df_pos_train.sample(n=n, random_state=1)\n",
    "    pos_tweets = df_pos_train['clean_text'].tolist()\n",
    "\n",
    "    df_neg_train = df_train[df_train['tweet_sentiment'] == 'negative']\n",
    "    #df_neg_train= df_neg_train.sample(n=n, random_state=1)\n",
    "    neg_tweets = df_neg_train['clean_text'].tolist()\n",
    "\n",
    "    df_neutral_train = df_train[df_train['tweet_sentiment'] == 'neutral']\n",
    "    #df_neutral_train= df_neutral_train.sample(n=n, random_state=1)\n",
    "    neutral_tweets = df_neutral_train['clean_text'].tolist()\n",
    "\n",
    "    \n",
    "\n",
    "    positive_featuresets = [(features(tweet),'positive') for tweet in pos_tweets]\n",
    "    negative_featuresets = [(features(tweet),'negative') for tweet in neg_tweets]\n",
    "    neutral_featuresets = [(features(tweet),'neutral') for tweet in neutral_tweets]\n",
    "    training_features = positive_featuresets + negative_featuresets + neutral_featuresets\n",
    "    ngram_vectorizer = CountVectorizer(analyzer = 'word',binary=True,lowercase = False, ngram_range=(1, 2))\n",
    "    ##train model\n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "    trainer = NaiveBayesClassifier.train\n",
    "\n",
    "    classifier = sentiment_analyzer.train(trainer, training_features)\n",
    "    truth_list = list(df_dev[['clean_text', 'tweet_sentiment']].itertuples(index=False, name=None))\n",
    "   \n",
    "    ##test model\n",
    "    for i, (text, expected) in enumerate(truth_list):\n",
    "        text_feats = features(text)\n",
    "        truth_list[i] = (text_feats, expected)\n",
    "    re=sentiment_analyzer.evaluate(truth_list,classifier)\n",
    "    print(re)\n",
    "    return classifier\n",
    "\n",
    "def features(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    return dict(('contains(%s)' % w, True) for w in words)\n",
    "\n",
    "##logistic regression\n",
    "def Logreg(df_train,df_dev):\n",
    "   \n",
    "    df_train['clean_text']=df_train['clean_text'].apply(lambda x: stem_stop(x))\n",
    "    df_dev['clean_text']=df_dev['clean_text'].apply(lambda x: stem_stop(x))\n",
    "    datat=df_train[['clean_text','tweet_sentiment']]\n",
    "    \n",
    "    ngram_vectorizer = CountVectorizer(analyzer = 'word',binary=True,lowercase = False, ngram_range=(1, 2))\n",
    "\n",
    "    y_train=datat['tweet_sentiment']\n",
    "   # log_model = LogisticRegression()\n",
    "   # log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    datatest=df_dev[['clean_text','tweet_sentiment']]\n",
    "    \n",
    "    X_train=ngram_vectorizer.fit_transform(df_train['clean_text'])\n",
    "    X_test = ngram_vectorizer.transform(df_dev['clean_text'])\n",
    "    \n",
    "    y_test=datatest['tweet_sentiment']\n",
    "    ##find the best parameter: cf\n",
    "    cf=0\n",
    "    accutemp=0\n",
    "    for c in [0.01, 0.05, 0.25, 0.3,0.5, 1]:\n",
    "    \n",
    "        lr = LogisticRegression(multi_class='multinomial', solver='newton-cg',C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        accu=accuracy_score(y_test, y_pred)\n",
    "        if accu>accutemp:\n",
    "            cf=c\n",
    "    lr = LogisticRegression(multi_class='multinomial', solver='newton-cg',C=cf)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    f1=f1_score(y_test, y_pred, average=None)\n",
    "    recall=recall_score(y_test, y_pred, average=None)\n",
    "    precision=precision_score(y_test, y_pred, average=None) \n",
    "    print('the f1 scores for negative, netural, postive are : ' ,f1)\n",
    "    print('the recall for negative, netural, postive are : ',recall)\n",
    "    print('the precision for negative, netural, postive are : ', precision)\n",
    "    return lr\n",
    "def main():\n",
    "    ##processing train and test dataset to csv\n",
    "    filelist=['devtest.txt','dev.txt','train.txt','test.txt']\n",
    "    outputlist=['devtest.csv','dev.csv','train.csv','test.csv']\n",
    "    for i in range(len(filelist)):\n",
    "        output='/data/Gold/'+outputlist[i]\n",
    "        file='/data/Gold/'+filelist[i]\n",
    "        pre_process_for_model(file,output)\n",
    "    ##load data\n",
    "    train = pd.read_csv(\"/data/Gold/train.csv\")\n",
    "    dev=pd.read_csv(\"/data/Gold/dev.csv\")\n",
    "    test=pd.read_csv(\"/data/Gold/test.csv\")\n",
    "    devtest=pd.read_csv(\"/data/Gold/devtest.csv\")\n",
    "    \n",
    "    \n",
    "##combine train and validation for get larger train dataset\n",
    "    frames = [train, dev]\n",
    "    train = pd.concat(frames)\n",
    "\n",
    "##remove highly frequent data\n",
    "    freq = pd.Series(' '.join(train['clean_text']).split()).value_counts()[:10]\n",
    "    freq = list(freq.index)\n",
    "    train['clean_text'] = train['clean_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    train['clean_text'].head()\n",
    "\n",
    "##remove rarely frequent data\n",
    "    freq = pd.Series(' '.join(train['clean_text']).split()).value_counts()[-10:]\n",
    "    freq = list(freq.index)\n",
    "    train['clean_text'] = train['clean_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    train['clean_text'].head()\n",
    "    ##train and validate to get optimal model\n",
    "    NB(train,devtest)\n",
    "    Logreg(train,devtest)\n",
    "    \n",
    "##based results from navie bayes and logistic regression, the result from logistic regression \n",
    "    Logre=Logreg(train,test)\n",
    "\n",
    "##used logstic regression for classify input data \n",
    "    ifile='/data/Dev/INPUT.txt'\n",
    "    output='/data/Dev/INPUT.csv'\n",
    "    pre_process_for_model(ifile,output)\n",
    "    inputdata=pd.read_csv(\"/data/Dev/INPUT.csv\")\n",
    "    \n",
    "    inputdata['clean_text']=inputdata['clean_text'].apply(lambda x: stem_stop(x))\n",
    "    \n",
    "    ngram_vectorizer = CountVectorizer(analyzer = 'word',binary=True,lowercase = False, ngram_range=(1, 2))\n",
    "    X_train=ngram_vectorizer.fit_transform(train['clean_text'])\n",
    "    X_test = ngram_vectorizer.transform(inputdata['clean_text'])\n",
    "    \n",
    "    y_pred = Logre.predict(X_test)\n",
    "    # List1  \n",
    "    ID = inputdata['tweet_id'] \n",
    "    \n",
    "# List2  \n",
    "    label = y_pred \n",
    "    \n",
    "# get the list of tuples from two lists.  \n",
    "# and merge them by using zip().  \n",
    "    list_of_tuples = list(zip(ID, label))  \n",
    "    \n",
    "# Assign data to tuples.  \n",
    "    list_of_tuples   \n",
    "  \n",
    "  \n",
    "# Converting lists of tuples into  \n",
    "# pandas Dataframe.  \n",
    "    df = pd.DataFrame(list_of_tuples, columns = ['ID', 'label'])  \n",
    "    df.to_csv(\"/data/Dev/Output.csv\")\n",
    "   ## y_pred\n",
    "  ##  label\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
